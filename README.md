# FinalYearProject

A brief description of the project, including its purpose and features.

## Table of Contents

1. [Introduction](#introduction)
2. [Folder Structure](#folder-structure)
3. [Getting Started](#getting-started)

## Introduction

This project is an example of using web scraping, machine learning (ML), and natural language processing (NLP) techniques to collect, clean, and analyze data. The data is stored in a database and can be used to answer questions, make predictions, or visualize trends.

## Folder Structure

This repository is organized into the following main folders:

1. `Data` - Contains all the CSV files used and saved throughout the web scraping process.
2. `Database` - Contains the database setup, testing, and configuration files.
3. `Models` - Contains the ML/NLP models, web scraping scripts, and data cleaning scripts.
4. `Archive` - Contains an archive of all files previously used but no longer needed.

### Data

This folder contains all the CSV files used and saved throughout the web scraping process. The data is collected from various sources and includes information relevant to the project's purpose and goals.

### Database

This folder contains the necessary files for setting up, testing, and configuring the project's database. The database is used to store and manage the data collected and processed by the project.

### Models

This folder contains the ML and NLP models used in the project, as well as the web scraping scripts and data cleaning scripts. These models and scripts are responsible for collecting, processing, and analyzing the data used in the project.

### Archive

This folder contains an archive of all files that were previously used but are no longer needed. This can include old versions of scripts, models, or data files that have since been updated or replaced.

## Getting Started

To get started with this project, follow these steps:

1. Clone the repository to your local machine.
2. Install any required dependencies (see the project's `requirements.txt` file for a list of required packages).
3. Set up and configure the database, as outlined in the `Database` folder's README file.
4. Run the web scraping and data cleaning scripts, as outlined in the `Models` folder's README file.
5. Train and evaluate the ML/NLP models, as outlined in the `Models` folder's README file.


